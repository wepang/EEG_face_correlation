{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 嘴部、眼部、表情检测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras == 2.3.1\n",
    "mediapipe == 0.0.10.1\n",
    "numpy == 1.21.6\n",
    "opencv-python == 4.2.0.32\n",
    "python == 3.7.12\n",
    "scipy == 1.4.1\n",
    "tenorflow-gpu == 2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from scipy.spatial import distance as dist\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.utils import image_utils\n",
    "import imutils\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#facial expression model\n",
    "emotion_model_path = 'vgg_mode.h5.' #model path\n",
    "\n",
    "emotion_classifier = load_model(emotion_model_path, compile=False)\n",
    "EMOTIONS = [\"angry\" ,\"disgust\",\"scared\", \"happy\", \"sad\", \"surprised\",\n",
    " \"neutral\"]\n",
    " \n",
    "mp_face_detection = mp.solutions.face_detection \n",
    "face_detection = mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n",
    "#mediapipe\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "model = mp_face_mesh.FaceMesh(   \n",
    "        static_image_mode=False,     \n",
    "        refine_landmarks=True,       \n",
    "        max_num_faces=5,              \n",
    "        min_detection_confidence=0.5, \n",
    "        min_tracking_confidence=0.5,  \n",
    ")\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1, color=[66,77,229])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'video root path'\n",
    "video_path = root_path + '/video' #students video path\n",
    "excel_path = root_path + '/faceori' #saved excel path\n",
    "video_file = os.listdir(video_path)\n",
    "table=0\n",
    "for video in video_file:\n",
    "    table+=1\n",
    "    input_path=video_path+'/'+video\n",
    "    #print(input_path)\n",
    "    print('start',input_path)\n",
    "        \n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    frame_count = 0\n",
    "    while(cap.isOpened()):\n",
    "        success, frame = cap.read()\n",
    "        frame_count += 1\n",
    "        if not success:\n",
    "            break\n",
    "    cap.release()\n",
    "    print('frames sum:',frame_count)\n",
    "\n",
    "    #导入视频\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    frame_size = (cap.get(cv2.CAP_PROP_FRAME_WIDTH), cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_width,frame_height = cap.get(cv2.CAP_PROP_FRAME_WIDTH), cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Divided into two parts, the first part obtains the landmarks of each frame, and the second part of each frame is cropped to obtain the predicted expressions\n",
    "    earlist = []\n",
    "    mearlist = []\n",
    "    emotionlist = []\n",
    "    try:\n",
    "        while(cap.isOpened()):\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                    break   \n",
    "\n",
    "            try:\n",
    "                h,w = frame.shape[0], frame.shape[1]\n",
    "                img_RGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                results = model.process(img_RGB)\n",
    "                \n",
    "                if results.multi_face_landmarks: # if detected faces\n",
    "\n",
    "                    F1 = results.multi_face_landmarks[0].landmark[33]; FT_X, FT_Y = int(F1.x * w), int(F1.y * h);   F1 = np.array([FT_X, FT_Y])\n",
    "                    F2 = results.multi_face_landmarks[0].landmark[160]; FT_X, FT_Y = int(F2.x * w), int(F2.y * h);  F2 = np.array([FT_X, FT_Y])\n",
    "                    F3 = results.multi_face_landmarks[0].landmark[159]; FT_X, FT_Y = int(F3.x * w), int(F3.y * h);  F3 = np.array([FT_X, FT_Y])\n",
    "                    F4 = results.multi_face_landmarks[0].landmark[158]; FT_X, FT_Y = int(F4.x * w), int(F4.y * h);  F4 = np.array([FT_X, FT_Y])\n",
    "                    F5 = results.multi_face_landmarks[0].landmark[157]; FT_X, FT_Y = int(F5.x * w), int(F5.y * h);  F5 = np.array([FT_X, FT_Y])\n",
    "                    F6 = results.multi_face_landmarks[0].landmark[133]; FT_X, FT_Y = int(F6.x * w), int(F6.y * h);  F6 = np.array([FT_X, FT_Y])\n",
    "                    F7 = results.multi_face_landmarks[0].landmark[154]; FT_X, FT_Y = int(F7.x * w), int(F7.y * h);  F7 = np.array([FT_X, FT_Y])\n",
    "                    F8 = results.multi_face_landmarks[0].landmark[153]; FT_X, FT_Y = int(F8.x * w), int(F8.y * h);  F8 = np.array([FT_X, FT_Y])\n",
    "                    F9 = results.multi_face_landmarks[0].landmark[145]; FT_X, FT_Y = int(F9.x * w), int(F9.y * h);  F9 = np.array([FT_X, FT_Y])\n",
    "                    F10 = results.multi_face_landmarks[0].landmark[144]; FT_X, FT_Y = int(F10.x * w), int(F10.y * h);  F10 = np.array([FT_X, FT_Y])\n",
    "                    F11 = results.multi_face_landmarks[0].landmark[362]; FT_X, FT_Y = int(F11.x * w), int(F11.y * h);  F11 = np.array([FT_X, FT_Y])\n",
    "                    F12 = results.multi_face_landmarks[0].landmark[384]; FT_X, FT_Y = int(F12.x * w), int(F12.y * h);  F12 = np.array([FT_X, FT_Y])\n",
    "                    F13 = results.multi_face_landmarks[0].landmark[385]; FT_X, FT_Y = int(F13.x * w), int(F13.y * h);  F13 = np.array([FT_X, FT_Y])\n",
    "                    F14 = results.multi_face_landmarks[0].landmark[386]; FT_X, FT_Y = int(F14.x * w), int(F14.y * h);  F14 = np.array([FT_X, FT_Y])\n",
    "                    F15 = results.multi_face_landmarks[0].landmark[387]; FT_X, FT_Y = int(F15.x * w), int(F15.y * h);  F15 = np.array([FT_X, FT_Y])\n",
    "                    F16 = results.multi_face_landmarks[0].landmark[263]; FT_X, FT_Y = int(F16.x * w), int(F16.y * h);  F16 = np.array([FT_X, FT_Y])\n",
    "                    F17 = results.multi_face_landmarks[0].landmark[373]; FT_X, FT_Y = int(F17.x * w), int(F17.y * h);  F17 = np.array([FT_X, FT_Y])\n",
    "                    F18 = results.multi_face_landmarks[0].landmark[374]; FT_X, FT_Y = int(F18.x * w), int(F18.y * h);  F18 = np.array([FT_X, FT_Y])\n",
    "                    F19 = results.multi_face_landmarks[0].landmark[380]; FT_X, FT_Y = int(F19.x * w), int(F19.y * h);  F19 = np.array([FT_X, FT_Y])\n",
    "                    F20 = results.multi_face_landmarks[0].landmark[381]; FT_X, FT_Y = int(F20.x * w), int(F20.y * h);  F20 = np.array([FT_X, FT_Y])\n",
    "                \n",
    "                    A = dist.euclidean(F2, F10)\n",
    "                    B = dist.euclidean(F3, F9)\n",
    "                    C = dist.euclidean(F4, F8)\n",
    "                    D = dist.euclidean(F5, F7)\n",
    "                    E = dist.euclidean(F1, F6)\n",
    "\n",
    "                \n",
    "                    F = dist.euclidean(F12, F20)\n",
    "                    G = dist.euclidean(F13, F19)\n",
    "                    H = dist.euclidean(F14, F18)\n",
    "                    I = dist.euclidean(F15, F17)\n",
    "                    J = dist.euclidean(F11, F16)\n",
    "                    \n",
    "                    # ear值\n",
    "                    ear_left = (A + B + C + D) / (4.0 * E)\n",
    "                    ear_right = (F + G + H + I) / (4.0 * J)\n",
    "                    ear = 0.5 * (ear_left + ear_right)\n",
    "                    earlist.append(ear)\n",
    "\n",
    "                    M1 = results.multi_face_landmarks[0].landmark[62]; FT_X, FT_Y = int(M1.x * w), int(M1.y * h); FT_Color = (31,41,81); M1 = np.array([FT_X, FT_Y])\n",
    "                    M2 = results.multi_face_landmarks[0].landmark[72]; FT_X, FT_Y = int(M2.x * w), int(M2.y * h); FT_Color = (31,41,81); M2 = np.array([FT_X, FT_Y])\n",
    "                    M3 = results.multi_face_landmarks[0].landmark[302]; FT_X, FT_Y = int(M3.x * w), int(M3.y * h); FT_Color = (31,41,81); M3 = np.array([FT_X, FT_Y])\n",
    "                    M4 = results.multi_face_landmarks[0].landmark[293]; FT_X, FT_Y = int(M4.x * w), int(M4.y * h); FT_Color = (31,41,81); M4 = np.array([FT_X, FT_Y])\n",
    "                    M5 = results.multi_face_landmarks[0].landmark[315]; FT_X, FT_Y = int(M5.x * w), int(M5.y * h); FT_Color = (31,41,81); M5 = np.array([FT_X, FT_Y])\n",
    "                    M6 = results.multi_face_landmarks[0].landmark[85]; FT_X, FT_Y = int(M6.x * w), int(M6.y * h); FT_Color = (31,41,81); M6 = np.array([FT_X, FT_Y])\n",
    "                    M7 = results.multi_face_landmarks[0].landmark[11]; FT_X, FT_Y = int(M7.x * w), int(M7.y * h);  M7 = np.array([FT_X, FT_Y])\n",
    "                    M8 = results.multi_face_landmarks[0].landmark[16]; FT_X, FT_Y = int(M8.x * w), int(M8.y * h);  M8 = np.array([FT_X, FT_Y])\n",
    "                \n",
    "                    K = dist.euclidean(M1, M4)\n",
    "                    L = dist.euclidean(M2, M6)\n",
    "                    M = dist.euclidean(M3, M5)\n",
    "                    N = dist.euclidean(M7, M8)\n",
    "                    \n",
    "                    # mear值\n",
    "                    mear = ( L + M + N) / (3.0 * K)\n",
    "                    mearlist.append(mear)\n",
    "                \n",
    "                else:\n",
    "                    mearlist.append(\"no\")\n",
    "                    earlist.append(\"no\")\n",
    "\n",
    "                # 下面是表情识别部分\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                face_detection_results = face_detection.process(frame)\n",
    "                if face_detection_results.detections:\n",
    "                    for face in face_detection_results.detections:\n",
    "                        face_box=face.location_data.relative_bounding_box\n",
    "\n",
    "                    (fX, fY, fW, fH) = int(face_box.xmin*frame_width),int(face_box.ymin*frame_height),int(face_box.width*frame_width),int(face_box.height*frame_height)\n",
    "                    (fX, fY, fW, fH) = fX-fW//4,fY-fH//4,fW*5//4,fH//4*5\n",
    "                    roi = gray[fY:fY + fH, fX:fX + fW]\n",
    "                    roi = cv2.resize(roi, (64, 64))\n",
    "                    roi = roi.astype(\"float\") / 255.0\n",
    "                    roi = image_utils.img_to_array(roi)\n",
    "                    roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "                    #表情识别是7分类问题，将概率值最大的作为表情输出\n",
    "                    preds = emotion_classifier.predict(roi)[0]\n",
    "                    emotion_probability = np.max(preds)\n",
    "                    label = EMOTIONS[preds.argmax()]\n",
    "                    emotionlist.append(label) \n",
    "                else:\n",
    "                    emotionlist.append(\"no\") \n",
    "                    continue\n",
    "            except:\n",
    "                print('error')\n",
    "                pass\n",
    "    except:\n",
    "        print('中途中断')\n",
    "        pass\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "        cap.release()\n",
    "\n",
    "    output_excel = {'ear':[],'mear':[],'emotion':[]}\n",
    "    output_excel['ear']=earlist\n",
    "    output_excel['mear'] = mearlist\n",
    "    output_excel['emotion'] = emotionlist\n",
    "    output = pd.DataFrame(output_excel)\n",
    "    fileName = os.path.splitext(video)[0]\n",
    "    output.to_csv(excel_path+'/'+fileName+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "root_path = 'EEGdata' #excel path\n",
    "ori_path =  root_path +'/faceori'\n",
    "output_path =  root_path + '/facedata_excel'\n",
    "files = os.listdir(ori_path)\n",
    "for  excel_file in files:\n",
    "    input_path=ori_path+'/'+excel_file\n",
    "\n",
    "    raw = pd.read_csv(input_path)\n",
    "    raw_data=raw.values\n",
    "    print(raw)\n",
    "\n",
    "    datalist={\"frame\":[],\"ear\":[],\"mear\":[],\"no\":[],\"angry\":[],\"happy\":[],\"neutral\":[],\"sad\":[],\"scared\":[],\"surprised\":[],\"disgust\":[],\"engagement\":[]}\n",
    "\n",
    "    for i in range(len(raw_data)):\n",
    "        datalist[\"frame\"].append(i+1)\n",
    "        datalist[\"ear\"].append(raw_data[i][0])\n",
    "        datalist[\"mear\"].append(raw_data[i][1])\n",
    "        datalist[\"no\"].append(0)\n",
    "        datalist[\"angry\"].append(0)\n",
    "        datalist[\"happy\"].append(0)\n",
    "        datalist[\"neutral\"].append(0)\n",
    "        datalist[\"sad\"].append(0)\n",
    "        datalist[\"scared\"].append(0)\n",
    "        datalist[\"surprised\"].append(0)\n",
    "        datalist[\"disgust\"].append(0)\n",
    "        datalist[\"engagement\"].append(0)\n",
    "        datalist[raw_data[i][2]][-1]=1\n",
    "        \n",
    "    output=pd.DataFrame(datalist)\n",
    "    fileName = os.path.splitext(excel_file)[0]\n",
    "    output.to_csv(output_path+'/'+fileName+'.csv',index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "991e2f69488ffc1a65da57a397a12078a5e5031fc49cedd7fe211fd659b35aa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

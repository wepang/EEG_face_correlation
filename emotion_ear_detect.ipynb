{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 嘴部、眼部、表情检测"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras == 2.3.1\n",
    "mediapipe == 0.0.10.1\n",
    "numpy == 1.21.6\n",
    "opencv-python == 4.2.0.32\n",
    "python == 3.7.12\n",
    "scipy == 1.4.1\n",
    "tenorflow-gpu == 2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mediapipe关键点检测所需package\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from scipy.spatial import distance as dist\n",
    "import pandas as pd\n",
    "import os\n",
    "#表情识别所需package\n",
    "#from keras.preprocessing.image import img_to_array\n",
    "from keras.utils import image_utils\n",
    "import imutils\n",
    "from keras.models import load_model\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#表情识别依赖模型配置\n",
    "emotion_model_path = 'D:/Research/Facedetection/face_mouth_emotion/_mini_XCEPTION.102-0.66.hdf5' #需要修改文件路径\n",
    "\n",
    "emotion_classifier = load_model(emotion_model_path, compile=False)\n",
    "EMOTIONS = [\"angry\" ,\"disgust\",\"scared\", \"happy\", \"sad\", \"surprised\",\n",
    " \"neutral\"]\n",
    " \n",
    "mp_face_detection = mp.solutions.face_detection \n",
    "face_detection = mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n",
    "\n",
    "#mediapipe依赖模型配置\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "model = mp_face_mesh.FaceMesh(   \n",
    "        static_image_mode=False,      # 是静态图片还是连续视频帧，摄像头画面为连续视频帧，此处选False\n",
    "        refine_landmarks=True,       # 使用注意力机制Attention Mesh Model，对嘴唇、眼睛、瞳孔周围的关键点精细定位\n",
    "        max_num_faces=5,              # 最多检测几张脸\n",
    "        min_detection_confidence=0.5, # 置信度阈值\n",
    "        min_tracking_confidence=0.5,  # 追踪阈值\n",
    ")\n",
    "# 导入可视化绘图函数\n",
    "mp_drawing = mp.solutions.drawing_utils \n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1, color=[66,77,229])\n",
    "\n",
    "\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(min_detection_confidence=0.5,\n",
    "                                  min_tracking_confidence=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_matrix_to_angles(rotation_matrix):\n",
    "    \"\"\"\n",
    "    Calculate Euler angles from rotation matrix.\n",
    "    :param rotation_matrix: A 3*3 matrix with the following structure\n",
    "    [Cosz*Cosy  Cosz*Siny*Sinx - Sinz*Cosx  Cosz*Siny*Cosx + Sinz*Sinx]\n",
    "    [Sinz*Cosy  Sinz*Siny*Sinx + Sinz*Cosx  Sinz*Siny*Cosx - Cosz*Sinx]\n",
    "    [  -Siny             CosySinx                   Cosy*Cosx         ]\n",
    "    :return: Angles in degrees for each axis\n",
    "    \"\"\"\n",
    "    x = math.atan2(rotation_matrix[2, 1], rotation_matrix[2, 2])\n",
    "    y = math.atan2(-rotation_matrix[2, 0], math.sqrt(rotation_matrix[0, 0] ** 2 +\n",
    "                                                     rotation_matrix[1, 0] ** 2))\n",
    "    z = math.atan2(rotation_matrix[1, 0], rotation_matrix[0, 0])\n",
    "    return np.array([x, y, z]) * 180. / math.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 5s 5s/step\n",
      "0.22117841652606057 0.08299096638287644 neutral 35.29173581724823 -9.703032053253244 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.11502795975810995 0.13961324541900183 sad 18.944256540645927 7.672349998550501 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.08296229802137847 0.11330053032699812 neutral 28.669888591816985 9.226874725700252 [2]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "0.06863527868306474 0.12010441714572119 sad 34.07947060365627 10.910070395877725 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.07300890084689252 0.13931570326068407 scared 40.710425727757865 12.61260412847682 [2]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "0.06952080857502937 0.15614751451148146 sad 35.76488913447382 12.336912533011725 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.14850100101560648 0.16979540892241224 sad 23.72500301693268 15.463992735944931 [2]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "0.2680232382922977 0.10164332553454962 sad 15.65882004164785 12.136685316959513 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.2578915389016464 0.09352984526076226 neutral 20.511146850726064 10.951846928006017 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.26987227627522914 0.09420840807213961 sad 25.08139751821246 10.284806725327583 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.28016824159715314 0.09380888215989241 neutral 25.004570565320144 8.861453214704232 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.27489764862195065 0.09079988252518263 neutral 23.667132358020513 8.646557362541737 [2]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "0.2817316582645465 0.07958233628950621 neutral 22.087672974025132 6.059386327553775 [2]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "0.2956059720222667 0.07900066050524944 sad 22.805187060009857 4.075574596131045 [2]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "0.28331009734621504 0.08297525756173657 sad 22.078658379240913 3.8007323060269576 [2]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "0.27761493306140056 0.0804506535764304 sad 20.96043251617168 2.7832128301338717 [2]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "0.2534153412254654 0.07763885421171181 neutral 20.59986838060881 1.268175466044744 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.2557893555339131 0.08292880876875472 sad 17.801611570652963 -0.022118038001117786 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.2640642710926965 0.07876475343517575 neutral 17.134827168534287 1.5125302383948422 [2]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "0.2716441301132885 0.07446873157757737 neutral 14.011584602445376 0.602807478108078 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.2760727220176108 0.07672038729082295 neutral 12.287453604499191 0.08707282915311922 [2]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "0.2853561093447712 0.07654868247924325 neutral 12.298268872572233 -0.03435288174811716 [2]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "0.29428309871214176 0.07814076773308162 neutral 11.118804561257198 0.5227278254025849 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.30685721390883136 0.08395011652093554 neutral 17.98124964795098 2.2014398290012416 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.3057097809828002 0.08233963566009914 neutral 17.79525591173041 0.9611260643871372 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.30087346199521414 0.08218212118734822 neutral 21.037849195557737 1.7412553574150471 [2]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "0.299805664166317 0.08214034106642217 neutral 22.748796721219293 1.5158037208572568 [2]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "0.27606758738144 0.07109321032766082 neutral 21.4338799494801 -0.5956585704846108 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.30228512916502515 0.07530381556239844 neutral 22.885982846703087 1.2614945102702821 [2]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "0.306524426908801 0.08023711700518954 neutral 21.39050259389579 1.5983883365973668 [2]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "0.2875237438480593 0.08118266149943158 neutral 18.501788824497076 2.2384314424866805 [2]\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "0.2982434216245371 0.08573447574594444 neutral 18.220143028298093 2.430978659593269 [2]\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "0.30172989900627734 0.08552087961441912 neutral 19.039803037025127 1.77772301553256 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.31452578391785574 0.08777011109678898 neutral 19.985723311084378 3.1253131236615186 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.2828718922052307 0.08480550234592744 neutral 17.955511240082213 3.223392557069274 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.2718358768761331 0.08716889064968755 neutral 19.313943377750316 3.652504100857208 [2]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "0.2696544297930805 0.08787605584684599 neutral 21.188222332176604 3.579643757793114 [2]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "0.2663452105868451 0.08681377446061547 neutral 23.419576469306573 4.5724693829897225 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.27187483914672816 0.08379700764239638 neutral 24.548240355074125 4.568304261409749 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.28249378225176375 0.08452472941975889 scared 24.3205168573327 4.272417439767043 [2]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "0.27082170203999234 0.09105292170257064 neutral 16.24401254139695 7.943543027437327 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.2797945852796751 0.09215475483573661 neutral 15.686247846252348 9.87718729165283 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.2671174525458176 0.09137642539109131 neutral 6.295802906894829 10.772839505717362 [2]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "0.2581731956450889 0.08622163702675063 neutral -3.4410994654505056 10.777848155002859 [2]\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "0.23136783349575063 0.09009797791403618 neutral -19.615293612659958 13.96575879007633 [2]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "0.21770669251783775 0.08673054403811435 neutral -22.21312827953135 14.08581687729624 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.2301495924870215 0.08508505389820409 neutral -23.375947401057346 14.126020983851038 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.270340834779448 0.08077743728916416 angry -24.605119357703465 10.506149897753653 [2]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "0.2876726515016258 0.08347284830958866 angry -22.27760359989408 7.730617467546428 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.2771097134353377 0.08255702912687254 angry -20.34558686985941 5.048237822178769 [2]\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "0.25493245204870374 0.08947244533658587 angry -18.23313816840489 4.7216319052758715 [2]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "0.2674742847142826 0.0865077914841196 neutral -1.9014071832826482 1.0859756321653342 [2]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "0.2941837266515443 0.08362604758936468 sad 7.5546313066958195 0.871557929146498 [2]\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "0.2977082816282626 0.084491476067657 neutral 15.177535507297968 0.6521952678500789 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.2774303013359997 0.07564848170142567 neutral 22.977345785028355 -2.6633853921159387 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.2872848258055855 0.07817926211388189 neutral 25.825772553911623 -4.028664214945279 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.28403700356047534 0.077159039040426 neutral 26.522582380897088 -4.833585228473181 [2]\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "0.27790212984212426 0.07786648615206138 neutral 27.987771451502404 -7.637129224304562 [2]\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "0.27440387308157577 0.08148361834192595 neutral 28.81030428336071 -7.571522477320591 [2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "frame_size = (cap.get(cv2.CAP_PROP_FRAME_WIDTH), cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_width,frame_height = cap.get(cv2.CAP_PROP_FRAME_WIDTH), cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    # Convert the color space from BGR to RGB and get Mediapipe results\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(image)\n",
    "    # Convert the color space from RGB to BGR to display well with Opencv\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    face_coordination_in_real_world = np.array([\n",
    "        [285, 528, 200],\n",
    "        [285, 371, 152],\n",
    "        [197, 574, 128],\n",
    "        [173, 425, 108],\n",
    "        [360, 574, 128],\n",
    "        [391, 425, 108]\n",
    "    ], dtype=np.float64)\n",
    "\n",
    "    h, w, _ = image.shape\n",
    "    face_coordination_in_image = []\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            for idx, lm in enumerate(face_landmarks.landmark):\n",
    "                if idx in [1, 9, 57, 130, 287, 359]:\n",
    "                    x, y = int(lm.x * w), int(lm.y * h)\n",
    "                    face_coordination_in_image.append([x, y])\n",
    "\n",
    "            face_coordination_in_image = np.array(face_coordination_in_image,\n",
    "                                                  dtype=np.float64)\n",
    "\n",
    "            # The camera matrix\n",
    "            focal_length = 1 * w\n",
    "            cam_matrix = np.array([[focal_length, 0, w / 2],\n",
    "                                   [0, focal_length, h / 2],\n",
    "                                   [0, 0, 1]])\n",
    "\n",
    "            # The Distance Matrix\n",
    "            dist_matrix = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "            # Use solvePnP function to get rotation vector\n",
    "            success, rotation_vec, transition_vec = cv2.solvePnP(\n",
    "                face_coordination_in_real_world, face_coordination_in_image,\n",
    "                cam_matrix, dist_matrix)\n",
    "\n",
    "            # Use Rodrigues function to convert rotation vector to matrix\n",
    "            rotation_matrix, jacobian = cv2.Rodrigues(rotation_vec)\n",
    "\n",
    "            result = rotation_matrix_to_angles(rotation_matrix)\n",
    "            pitch = result[0]\n",
    "            yaw = result[1]\n",
    "            roll = result[2]\n",
    "            for i, info in enumerate(zip(('pitch', 'yaw', 'roll'), result)):\n",
    "                k, v = info\n",
    "                #print(k,\"here\",v)\n",
    "                text = f'{k}: {int(v)}'\n",
    "                cv2.putText(image, text, (20, i*30 + 20),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 0, 200), 2)\n",
    "        F1 = results.multi_face_landmarks[0].landmark[33]; FT_X, FT_Y = int(F1.x * w), int(F1.y * h);   F1 = np.array([FT_X, FT_Y])\n",
    "        F2 = results.multi_face_landmarks[0].landmark[160]; FT_X, FT_Y = int(F2.x * w), int(F2.y * h);  F2 = np.array([FT_X, FT_Y])\n",
    "        F3 = results.multi_face_landmarks[0].landmark[159]; FT_X, FT_Y = int(F3.x * w), int(F3.y * h);  F3 = np.array([FT_X, FT_Y])\n",
    "        F4 = results.multi_face_landmarks[0].landmark[158]; FT_X, FT_Y = int(F4.x * w), int(F4.y * h);  F4 = np.array([FT_X, FT_Y])\n",
    "        F5 = results.multi_face_landmarks[0].landmark[157]; FT_X, FT_Y = int(F5.x * w), int(F5.y * h);  F5 = np.array([FT_X, FT_Y])\n",
    "        F6 = results.multi_face_landmarks[0].landmark[133]; FT_X, FT_Y = int(F6.x * w), int(F6.y * h);  F6 = np.array([FT_X, FT_Y])\n",
    "        F7 = results.multi_face_landmarks[0].landmark[154]; FT_X, FT_Y = int(F7.x * w), int(F7.y * h);  F7 = np.array([FT_X, FT_Y])\n",
    "        F8 = results.multi_face_landmarks[0].landmark[153]; FT_X, FT_Y = int(F8.x * w), int(F8.y * h);  F8 = np.array([FT_X, FT_Y])\n",
    "        F9 = results.multi_face_landmarks[0].landmark[145]; FT_X, FT_Y = int(F9.x * w), int(F9.y * h);  F9 = np.array([FT_X, FT_Y])\n",
    "        F10 = results.multi_face_landmarks[0].landmark[144]; FT_X, FT_Y = int(F10.x * w), int(F10.y * h);  F10 = np.array([FT_X, FT_Y])\n",
    "        F11 = results.multi_face_landmarks[0].landmark[362]; FT_X, FT_Y = int(F11.x * w), int(F11.y * h);  F11 = np.array([FT_X, FT_Y])\n",
    "        F12 = results.multi_face_landmarks[0].landmark[384]; FT_X, FT_Y = int(F12.x * w), int(F12.y * h);  F12 = np.array([FT_X, FT_Y])\n",
    "        F13 = results.multi_face_landmarks[0].landmark[385]; FT_X, FT_Y = int(F13.x * w), int(F13.y * h);  F13 = np.array([FT_X, FT_Y])\n",
    "        F14 = results.multi_face_landmarks[0].landmark[386]; FT_X, FT_Y = int(F14.x * w), int(F14.y * h);  F14 = np.array([FT_X, FT_Y])\n",
    "        F15 = results.multi_face_landmarks[0].landmark[387]; FT_X, FT_Y = int(F15.x * w), int(F15.y * h);  F15 = np.array([FT_X, FT_Y])\n",
    "        F16 = results.multi_face_landmarks[0].landmark[263]; FT_X, FT_Y = int(F16.x * w), int(F16.y * h);  F16 = np.array([FT_X, FT_Y])\n",
    "        F17 = results.multi_face_landmarks[0].landmark[373]; FT_X, FT_Y = int(F17.x * w), int(F17.y * h);  F17 = np.array([FT_X, FT_Y])\n",
    "        F18 = results.multi_face_landmarks[0].landmark[374]; FT_X, FT_Y = int(F18.x * w), int(F18.y * h);  F18 = np.array([FT_X, FT_Y])\n",
    "        F19 = results.multi_face_landmarks[0].landmark[380]; FT_X, FT_Y = int(F19.x * w), int(F19.y * h);  F19 = np.array([FT_X, FT_Y])\n",
    "        F20 = results.multi_face_landmarks[0].landmark[381]; FT_X, FT_Y = int(F20.x * w), int(F20.y * h);  F20 = np.array([FT_X, FT_Y])\n",
    "    \n",
    "        A = dist.euclidean(F2, F10)\n",
    "        B = dist.euclidean(F3, F9)\n",
    "        C = dist.euclidean(F4, F8)\n",
    "        D = dist.euclidean(F5, F7)\n",
    "        E = dist.euclidean(F1, F6)\n",
    "\n",
    "    \n",
    "        F = dist.euclidean(F12, F20)\n",
    "        G = dist.euclidean(F13, F19)\n",
    "        H = dist.euclidean(F14, F18)\n",
    "        I = dist.euclidean(F15, F17)\n",
    "        J = dist.euclidean(F11, F16)\n",
    "        \n",
    "        # ear值\n",
    "        ear_left = (A + B + C + D) / (4.0 * E)\n",
    "        ear_right = (F + G + H + I) / (4.0 * J)\n",
    "        ear = 0.5 * (ear_left + ear_right)\n",
    "\n",
    "        M1 = results.multi_face_landmarks[0].landmark[62]; FT_X, FT_Y = int(M1.x * w), int(M1.y * h); FT_Color = (31,41,81); M1 = np.array([FT_X, FT_Y])\n",
    "        M2 = results.multi_face_landmarks[0].landmark[72]; FT_X, FT_Y = int(M2.x * w), int(M2.y * h); FT_Color = (31,41,81); M2 = np.array([FT_X, FT_Y])\n",
    "        M3 = results.multi_face_landmarks[0].landmark[302]; FT_X, FT_Y = int(M3.x * w), int(M3.y * h); FT_Color = (31,41,81); M3 = np.array([FT_X, FT_Y])\n",
    "        M4 = results.multi_face_landmarks[0].landmark[293]; FT_X, FT_Y = int(M4.x * w), int(M4.y * h); FT_Color = (31,41,81); M4 = np.array([FT_X, FT_Y])\n",
    "        M5 = results.multi_face_landmarks[0].landmark[315]; FT_X, FT_Y = int(M5.x * w), int(M5.y * h); FT_Color = (31,41,81); M5 = np.array([FT_X, FT_Y])\n",
    "        M6 = results.multi_face_landmarks[0].landmark[85]; FT_X, FT_Y = int(M6.x * w), int(M6.y * h); FT_Color = (31,41,81); M6 = np.array([FT_X, FT_Y])\n",
    "        M7 = results.multi_face_landmarks[0].landmark[11]; FT_X, FT_Y = int(M7.x * w), int(M7.y * h);  M7 = np.array([FT_X, FT_Y])\n",
    "        M8 = results.multi_face_landmarks[0].landmark[16]; FT_X, FT_Y = int(M8.x * w), int(M8.y * h);  M8 = np.array([FT_X, FT_Y])\n",
    "    \n",
    "        K = dist.euclidean(M1, M4)\n",
    "        L = dist.euclidean(M2, M6)\n",
    "        M = dist.euclidean(M3, M5)\n",
    "        N = dist.euclidean(M7, M8)\n",
    "\n",
    "        \n",
    "        # mear值\n",
    "        mear = ( L + M + N) / (3.0 * K)\n",
    "\n",
    "        # 下面是表情识别部分\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        face_detection_results = face_detection.process(image)\n",
    "        if face_detection_results.detections:\n",
    "            for face in face_detection_results.detections:\n",
    "                face_box=face.location_data.relative_bounding_box\n",
    "\n",
    "            (fX, fY, fW, fH) = int(face_box.xmin*frame_width),int(face_box.ymin*frame_height),int(face_box.width*frame_width),int(face_box.height*frame_height)\n",
    "            (fX, fY, fW, fH) = fX-fW//4,fY-fH//4,fW*5//4,fH//4*5\n",
    "            roi = gray[fY:fY + fH, fX:fX + fW]\n",
    "            roi = cv2.resize(roi, (64, 64))\n",
    "            roi = roi.astype(\"float\") / 255.0\n",
    "            roi = image_utils.img_to_array(roi)\n",
    "            roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "            #表情识别是7分类问题，将概率值最大的作为表情输出\n",
    "            preds = emotion_classifier.predict(roi)[0]\n",
    "            emotion_probability = np.max(preds)\n",
    "            label = EMOTIONS[preds.argmax()]\n",
    "\n",
    "    print(ear,mear,label,yaw,pitch,roll)\n",
    "    cv2.imshow('Head Pose Angles', image)\n",
    "\n",
    "    if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'D:/Research/EEGdata/li'\n",
    "video_path = root_path + '/video'\n",
    "excel_path = root_path + '/faceori'\n",
    "video_file = os.listdir(video_path)\n",
    "table=0\n",
    "for video in video_file:\n",
    "    table+=1\n",
    "    input_path=video_path+'/'+video\n",
    "    #print(input_path)\n",
    "    #查看导入视频的总帧数\n",
    "    #input_path = 'C://TJ//detection//video//....mp4' #修改为本地的视频文件地址\n",
    "    print('视频开始处理',input_path)\n",
    "        \n",
    "    # 获取视频总帧数\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    frame_count = 0\n",
    "    while(cap.isOpened()):\n",
    "        success, frame = cap.read()\n",
    "        frame_count += 1\n",
    "        if not success:\n",
    "            break\n",
    "    cap.release()\n",
    "    print('视频总帧数为',frame_count)\n",
    "\n",
    "    #导入视频\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    frame_size = (cap.get(cv2.CAP_PROP_FRAME_WIDTH), cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    frame_width,frame_height = cap.get(cv2.CAP_PROP_FRAME_WIDTH), cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # 分为两部分，第一部分获取每一帧画面的关键点，第二部分每一帧画面进行剪裁获取预测表情\n",
    "    earlist = []\n",
    "    mearlist = []\n",
    "    emotionlist = []\n",
    "    try:\n",
    "        while(cap.isOpened()):\n",
    "            success, frame = cap.read()\n",
    "            if not success:\n",
    "                    break   \n",
    "\n",
    "            try:\n",
    "                h,w = frame.shape[0], frame.shape[1]\n",
    "                img_RGB = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                # 将RGB图像输入模型，获取预测结果\n",
    "                results = model.process(img_RGB)\n",
    "                \n",
    "                if results.multi_face_landmarks: # 如果检测出人脸\n",
    "\n",
    "                    F1 = results.multi_face_landmarks[0].landmark[33]; FT_X, FT_Y = int(F1.x * w), int(F1.y * h);   F1 = np.array([FT_X, FT_Y])\n",
    "                    F2 = results.multi_face_landmarks[0].landmark[160]; FT_X, FT_Y = int(F2.x * w), int(F2.y * h);  F2 = np.array([FT_X, FT_Y])\n",
    "                    F3 = results.multi_face_landmarks[0].landmark[159]; FT_X, FT_Y = int(F3.x * w), int(F3.y * h);  F3 = np.array([FT_X, FT_Y])\n",
    "                    F4 = results.multi_face_landmarks[0].landmark[158]; FT_X, FT_Y = int(F4.x * w), int(F4.y * h);  F4 = np.array([FT_X, FT_Y])\n",
    "                    F5 = results.multi_face_landmarks[0].landmark[157]; FT_X, FT_Y = int(F5.x * w), int(F5.y * h);  F5 = np.array([FT_X, FT_Y])\n",
    "                    F6 = results.multi_face_landmarks[0].landmark[133]; FT_X, FT_Y = int(F6.x * w), int(F6.y * h);  F6 = np.array([FT_X, FT_Y])\n",
    "                    F7 = results.multi_face_landmarks[0].landmark[154]; FT_X, FT_Y = int(F7.x * w), int(F7.y * h);  F7 = np.array([FT_X, FT_Y])\n",
    "                    F8 = results.multi_face_landmarks[0].landmark[153]; FT_X, FT_Y = int(F8.x * w), int(F8.y * h);  F8 = np.array([FT_X, FT_Y])\n",
    "                    F9 = results.multi_face_landmarks[0].landmark[145]; FT_X, FT_Y = int(F9.x * w), int(F9.y * h);  F9 = np.array([FT_X, FT_Y])\n",
    "                    F10 = results.multi_face_landmarks[0].landmark[144]; FT_X, FT_Y = int(F10.x * w), int(F10.y * h);  F10 = np.array([FT_X, FT_Y])\n",
    "                    F11 = results.multi_face_landmarks[0].landmark[362]; FT_X, FT_Y = int(F11.x * w), int(F11.y * h);  F11 = np.array([FT_X, FT_Y])\n",
    "                    F12 = results.multi_face_landmarks[0].landmark[384]; FT_X, FT_Y = int(F12.x * w), int(F12.y * h);  F12 = np.array([FT_X, FT_Y])\n",
    "                    F13 = results.multi_face_landmarks[0].landmark[385]; FT_X, FT_Y = int(F13.x * w), int(F13.y * h);  F13 = np.array([FT_X, FT_Y])\n",
    "                    F14 = results.multi_face_landmarks[0].landmark[386]; FT_X, FT_Y = int(F14.x * w), int(F14.y * h);  F14 = np.array([FT_X, FT_Y])\n",
    "                    F15 = results.multi_face_landmarks[0].landmark[387]; FT_X, FT_Y = int(F15.x * w), int(F15.y * h);  F15 = np.array([FT_X, FT_Y])\n",
    "                    F16 = results.multi_face_landmarks[0].landmark[263]; FT_X, FT_Y = int(F16.x * w), int(F16.y * h);  F16 = np.array([FT_X, FT_Y])\n",
    "                    F17 = results.multi_face_landmarks[0].landmark[373]; FT_X, FT_Y = int(F17.x * w), int(F17.y * h);  F17 = np.array([FT_X, FT_Y])\n",
    "                    F18 = results.multi_face_landmarks[0].landmark[374]; FT_X, FT_Y = int(F18.x * w), int(F18.y * h);  F18 = np.array([FT_X, FT_Y])\n",
    "                    F19 = results.multi_face_landmarks[0].landmark[380]; FT_X, FT_Y = int(F19.x * w), int(F19.y * h);  F19 = np.array([FT_X, FT_Y])\n",
    "                    F20 = results.multi_face_landmarks[0].landmark[381]; FT_X, FT_Y = int(F20.x * w), int(F20.y * h);  F20 = np.array([FT_X, FT_Y])\n",
    "                \n",
    "                    A = dist.euclidean(F2, F10)\n",
    "                    B = dist.euclidean(F3, F9)\n",
    "                    C = dist.euclidean(F4, F8)\n",
    "                    D = dist.euclidean(F5, F7)\n",
    "                    E = dist.euclidean(F1, F6)\n",
    "\n",
    "                \n",
    "                    F = dist.euclidean(F12, F20)\n",
    "                    G = dist.euclidean(F13, F19)\n",
    "                    H = dist.euclidean(F14, F18)\n",
    "                    I = dist.euclidean(F15, F17)\n",
    "                    J = dist.euclidean(F11, F16)\n",
    "                    \n",
    "                    # ear值\n",
    "                    ear_left = (A + B + C + D) / (4.0 * E)\n",
    "                    ear_right = (F + G + H + I) / (4.0 * J)\n",
    "                    ear = 0.5 * (ear_left + ear_right)\n",
    "                    earlist.append(ear)\n",
    "\n",
    "                    M1 = results.multi_face_landmarks[0].landmark[62]; FT_X, FT_Y = int(M1.x * w), int(M1.y * h); FT_Color = (31,41,81); M1 = np.array([FT_X, FT_Y])\n",
    "                    M2 = results.multi_face_landmarks[0].landmark[72]; FT_X, FT_Y = int(M2.x * w), int(M2.y * h); FT_Color = (31,41,81); M2 = np.array([FT_X, FT_Y])\n",
    "                    M3 = results.multi_face_landmarks[0].landmark[302]; FT_X, FT_Y = int(M3.x * w), int(M3.y * h); FT_Color = (31,41,81); M3 = np.array([FT_X, FT_Y])\n",
    "                    M4 = results.multi_face_landmarks[0].landmark[293]; FT_X, FT_Y = int(M4.x * w), int(M4.y * h); FT_Color = (31,41,81); M4 = np.array([FT_X, FT_Y])\n",
    "                    M5 = results.multi_face_landmarks[0].landmark[315]; FT_X, FT_Y = int(M5.x * w), int(M5.y * h); FT_Color = (31,41,81); M5 = np.array([FT_X, FT_Y])\n",
    "                    M6 = results.multi_face_landmarks[0].landmark[85]; FT_X, FT_Y = int(M6.x * w), int(M6.y * h); FT_Color = (31,41,81); M6 = np.array([FT_X, FT_Y])\n",
    "                    M7 = results.multi_face_landmarks[0].landmark[11]; FT_X, FT_Y = int(M7.x * w), int(M7.y * h);  M7 = np.array([FT_X, FT_Y])\n",
    "                    M8 = results.multi_face_landmarks[0].landmark[16]; FT_X, FT_Y = int(M8.x * w), int(M8.y * h);  M8 = np.array([FT_X, FT_Y])\n",
    "                \n",
    "                    K = dist.euclidean(M1, M4)\n",
    "                    L = dist.euclidean(M2, M6)\n",
    "                    M = dist.euclidean(M3, M5)\n",
    "                    N = dist.euclidean(M7, M8)\n",
    "                    \n",
    "                    # mear值\n",
    "                    mear = ( L + M + N) / (3.0 * K)\n",
    "                    mearlist.append(mear)\n",
    "                \n",
    "                else:\n",
    "                    mearlist.append(\"no\")\n",
    "                    earlist.append(\"no\")\n",
    "\n",
    "                # 下面是表情识别部分\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                face_detection_results = face_detection.process(frame)\n",
    "                if face_detection_results.detections:\n",
    "                    for face in face_detection_results.detections:\n",
    "                        face_box=face.location_data.relative_bounding_box\n",
    "\n",
    "                    (fX, fY, fW, fH) = int(face_box.xmin*frame_width),int(face_box.ymin*frame_height),int(face_box.width*frame_width),int(face_box.height*frame_height)\n",
    "                    (fX, fY, fW, fH) = fX-fW//4,fY-fH//4,fW*5//4,fH//4*5\n",
    "                    roi = gray[fY:fY + fH, fX:fX + fW]\n",
    "                    roi = cv2.resize(roi, (64, 64))\n",
    "                    roi = roi.astype(\"float\") / 255.0\n",
    "                    roi = image_utils.img_to_array(roi)\n",
    "                    roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "                    #表情识别是7分类问题，将概率值最大的作为表情输出\n",
    "                    preds = emotion_classifier.predict(roi)[0]\n",
    "                    emotion_probability = np.max(preds)\n",
    "                    label = EMOTIONS[preds.argmax()]\n",
    "                    emotionlist.append(label) \n",
    "                else:\n",
    "                    emotionlist.append(\"no\") \n",
    "                    continue\n",
    "            except:\n",
    "                print('error')\n",
    "                pass\n",
    "    except:\n",
    "        print('中途中断')\n",
    "        pass\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "        cap.release()\n",
    "\n",
    "    output_excel = {'ear':[],'mear':[],'emotion':[]}\n",
    "    output_excel['ear']=earlist\n",
    "    output_excel['mear'] = mearlist\n",
    "    output_excel['emotion'] = emotionlist\n",
    "    output = pd.DataFrame(output_excel)\n",
    "    fileName = os.path.splitext(video)[0]\n",
    "    output.to_csv(excel_path+'/'+fileName+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "root_path = 'D:/Research/EEGdata/li'\n",
    "ori_path =  root_path +'/faceori'\n",
    "output_path =  root_path + '/facedata_excel'\n",
    "files = os.listdir(ori_path)\n",
    "for  excel_file in files:\n",
    "    input_path=ori_path+'/'+excel_file\n",
    "\n",
    "    raw = pd.read_csv(input_path)\n",
    "    raw_data=raw.values\n",
    "    print(raw)\n",
    "\n",
    "    datalist={\"frame\":[],\"ear\":[],\"mear\":[],\"no\":[],\"angry\":[],\"happy\":[],\"neutral\":[],\"sad\":[],\"scared\":[],\"surprised\":[],\"disgust\":[],\"engagement\":[]}\n",
    "\n",
    "    for i in range(len(raw_data)):\n",
    "        datalist[\"frame\"].append(i+1)\n",
    "        datalist[\"ear\"].append(raw_data[i][0])\n",
    "        datalist[\"mear\"].append(raw_data[i][1])\n",
    "        datalist[\"no\"].append(0)\n",
    "        datalist[\"angry\"].append(0)\n",
    "        datalist[\"happy\"].append(0)\n",
    "        datalist[\"neutral\"].append(0)\n",
    "        datalist[\"sad\"].append(0)\n",
    "        datalist[\"scared\"].append(0)\n",
    "        datalist[\"surprised\"].append(0)\n",
    "        datalist[\"disgust\"].append(0)\n",
    "        datalist[\"engagement\"].append(0)\n",
    "        datalist[raw_data[i][2]][-1]=1\n",
    "        \n",
    "    output=pd.DataFrame(datalist)\n",
    "    fileName = os.path.splitext(excel_file)[0]\n",
    "    output.to_csv(output_path+'/'+fileName+'.csv',index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "991e2f69488ffc1a65da57a397a12078a5e5031fc49cedd7fe211fd659b35aa6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
